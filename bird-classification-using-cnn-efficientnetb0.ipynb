{"cells":[{"cell_type":"markdown","id":"1fd9e307","metadata":{"id":"1fd9e307"},"source":["# üèóÔ∏èImport Necessary Libraries\n","\n","<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em\">The code imports various data science libraries, including TensorFlow and scikit-learn, and defines a neural network model using the EfficientNet architecture. It also includes system libraries for handling file paths and metrics for evaluating model performance. Additionally, it sets up various callbacks and optimizers to optimize the training process.</p>"]},{"cell_type":"code","execution_count":1,"id":"r354cvV3KFsu","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16342,"status":"ok","timestamp":1704318564476,"user":{"displayName":"Gilad Danin","userId":"00864302539574052919"},"user_tz":-120},"id":"r354cvV3KFsu","outputId":"0283db7a-80f2-47a3-e2db-b552637bc1df"},"outputs":[],"source":["#!pip install tensorflow\n","#!pip install opencv-python\n","#!pip install cv2\n","# !pip install BIRDS\n","# pip install --upgrade --user pip\n","# !pip install google.colab"]},{"cell_type":"code","execution_count":2,"id":"63f7cb36","metadata":{"executionInfo":{"elapsed":8670,"status":"ok","timestamp":1704318573140,"user":{"displayName":"Gilad Danin","userId":"00864302539574052919"},"user_tz":-120},"id":"63f7cb36"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From c:\\Users\\gilad\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n","\n"]}],"source":["# Import Data Science Libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from pathlib import Path\n","\n","# import birds_utils.BIRDS\n","from sklearn.model_selection import train_test_split\n","\n","# Tensorflow Libraries\n","from tensorflow import keras\n","from tensorflow.keras import layers,models\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.callbacks import Callback, EarlyStopping,ModelCheckpoint, ReduceLROnPlateau\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras import Model\n","from tensorflow.keras.layers.experimental import preprocessing\n","\n","# System libraries\n","from pathlib import Path\n","import os.path\n","import random\n","\n","# Visualization Libraries\n","import matplotlib.cm as cm\n","import cv2\n","import seaborn as sns\n","import birds_utils as birds\n","sns.set_style('darkgrid')\n","\n","# Metrics\n","from sklearn.metrics import classification_report, confusion_matrix\n","import itertools"]},{"cell_type":"code","execution_count":3,"id":"1a57292a","metadata":{},"outputs":[],"source":["# principle parameters\n","BATCH_SIZE = 32 # size of batch (The ImageDataGenerator objects generate batches of tensor image data with real-time data augmentation)\n","TARGET_SIZE = (224, 224)    # size of pictures (used by the ImageDataGenerator objects)\n","N_LABELS = 3    # number of lables for analysis"]},{"cell_type":"markdown","id":"ed25e03f","metadata":{"id":"ed25e03f"},"source":["# ü§ôCreate helper functions"]},{"cell_type":"code","execution_count":4,"id":"f86c7993","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1704318573141,"user":{"displayName":"Gilad Danin","userId":"00864302539574052919"},"user_tz":-120},"id":"f86c7993","outputId":"888c697b-45b6-416b-ed9a-8b5bfdde2fcd"},"outputs":[],"source":["# !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n","\n","# # Import series of helper functions for our notebook\n","# from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, compare_historys, walk_through_dir, pred_and_plot"]},{"cell_type":"code","execution_count":5,"id":"8f91b054","metadata":{},"outputs":[],"source":["# import urllib.request\n","\n","# url = \"https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\"\n","# filename = \"helper_functions.py\"\n","# urllib.request.urlretrieve(url, filename)"]},{"cell_type":"markdown","id":"eaf7aa03","metadata":{"id":"eaf7aa03"},"source":["# üì•Load and Transform Data"]},{"cell_type":"code","execution_count":6,"id":"ee9bb3f0","metadata":{},"outputs":[],"source":["project_dir = '.'\n","image_df_full = birds.load_data(project_dir)  "]},{"cell_type":"code","execution_count":7,"id":"4b6821dd","metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'filter_df' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\gilad\\courses\\AI course\\final_project\\birds\\bird-classification-using-cnn-efficientnetb0.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/gilad/courses/AI%20course/final_project/birds/bird-classification-using-cnn-efficientnetb0.ipynb#Y140sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m lables \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(birds\u001b[39m.\u001b[39mget_labels(image_df_full)[\u001b[39m0\u001b[39m:N_LABELS])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/gilad/courses/AI%20course/final_project/birds/bird-classification-using-cnn-efficientnetb0.ipynb#Y140sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m image_df \u001b[39m=\u001b[39m filter_df(image_df_full,labels\u001b[39m=\u001b[39mlables)\n","\u001b[1;31mNameError\u001b[0m: name 'filter_df' is not defined"]}],"source":["lables = list(birds.get_labels(image_df_full)[0:N_LABELS])\n","image_df = birds.filter_df(image_df_full,labels=lables)"]},{"cell_type":"markdown","id":"2e4b4f52","metadata":{"id":"2e4b4f52"},"source":["# üî≠Visualizing images from the dataset"]},{"cell_type":"code","execution_count":null,"id":"6b463763","metadata":{},"outputs":[],"source":["# birds.image_df\n","# plot all images of a label\n","# birds.plot_label_images(label=birds.get_labels()[1])\n","\n","# plot the 1'st 5 instances of a label\n","# birds.plot_label_images(image_df,label=birds.get_labels(image_df)[1],N=5)\n","\n","# # plot images according to idx\n","# birds.plot_label_images(idx=[1,9])\n","\n","birds.plot_labels_count(image_df)"]},{"cell_type":"markdown","id":"65aa4c2e","metadata":{"id":"65aa4c2e"},"source":["# üìùData Preprocessing\n","<div style=\"background-color:#fff1cc; padding: 20px;\">\n","<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em\">The data will be split into three different categories: Training, Validation and Testing. The training data will be used to train the deep learning CNN model and its parameters will be fine tuned with the validation data. Finally, the performance of the data will be evaluated using the test data(data the model has not previously seen).</p>\n","</div>"]},{"cell_type":"code","execution_count":null,"id":"dbab3213","metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1704313348502,"user":{"displayName":"Gilad Danin","userId":"00864302539574052919"},"user_tz":-120},"id":"dbab3213"},"outputs":[],"source":["# Separate in train and test data\n","train_df, test_df = train_test_split(image_df, test_size=0.2, shuffle=True, random_state=42)"]},{"cell_type":"code","execution_count":null,"id":"c1c53678","metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1704313348502,"user":{"displayName":"Gilad Danin","userId":"00864302539574052919"},"user_tz":-120},"id":"c1c53678"},"outputs":[],"source":["train_generator = ImageDataGenerator(\n","    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input,\n","    validation_split=0.2\n",")\n","\n","test_generator = ImageDataGenerator(\n","    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input,\n",")"]},{"cell_type":"code","execution_count":null,"id":"576c78fb","metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1704313348503,"user":{"displayName":"Gilad Danin","userId":"00864302539574052919"},"user_tz":-120},"id":"576c78fb"},"outputs":[],"source":["# Split the data into three categories.\n","train_images = train_generator.flow_from_dataframe(\n","    dataframe=train_df,\n","    x_col='Filepath',\n","    y_col='Label',\n","    target_size=TARGET_SIZE,\n","    color_mode='rgb',\n","    class_mode='categorical',\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    seed=42,\n","    subset='training'\n",")\n","\n","val_images = train_generator.flow_from_dataframe(\n","    dataframe=train_df,\n","    x_col='Filepath',\n","    y_col='Label',\n","    target_size=TARGET_SIZE,\n","    color_mode='rgb',\n","    class_mode='categorical',\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    seed=42,\n","    subset='validation'\n",")\n","\n","test_images = test_generator.flow_from_dataframe(\n","    dataframe=test_df,\n","    x_col='Filepath',\n","    y_col='Label',\n","    target_size=TARGET_SIZE,\n","    color_mode='rgb',\n","    class_mode='categorical',\n","    batch_size=BATCH_SIZE,\n","    shuffle=False\n",")"]},{"cell_type":"code","execution_count":null,"id":"e4085df3","metadata":{"executionInfo":{"elapsed":73565,"status":"aborted","timestamp":1704313348503,"user":{"displayName":"Gilad Danin","userId":"00864302539574052919"},"user_tz":-120},"id":"e4085df3"},"outputs":[],"source":["# Data Augmentation Step\n","augment = tf.keras.Sequential([\n","  layers.experimental.preprocessing.Resizing(224,224),\n","  layers.experimental.preprocessing.Rescaling(1./255),\n","  layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n","  layers.experimental.preprocessing.RandomRotation(0.1),\n","  layers.experimental.preprocessing.RandomZoom(0.1),\n","  layers.experimental.preprocessing.RandomContrast(0.1),\n","])"]},{"cell_type":"markdown","id":"0dab2d1c","metadata":{"id":"0dab2d1c"},"source":["# ü§πTraining the model\n","<div style=\"background-color:#fff1cc; padding: 20px;\">\n","  <p style=\"font-size:20px; font-family:verdana; line-height: 1.7em\">The model images will be subjected to a pre-trained CNN model called EfficientNetB0. Three callbacks will be utilized to monitor the training. These are: Model Checkpoint, Early Stopping, Tensorboard callback. The summary of the model hyperparameter is shown as follows:</p>\n","\n","  <p style=\"font-size:20px\">\n","    <strong>Batch size</strong>: 32<br>\n","    <strong>Epochs</strong>: 100<br>\n","    <strong>Input Shape</strong>: (224, 224, 3)<br>\n","    <strong>Output layer</strong>: 525\n","  </p>\n","</div>\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"2f4051af","metadata":{"executionInfo":{"elapsed":73565,"status":"aborted","timestamp":1704313348503,"user":{"displayName":"Gilad Danin","userId":"00864302539574052919"},"user_tz":-120},"id":"2f4051af"},"outputs":[],"source":["# Load the pretained model\n","pretrained_model = tf.keras.applications.efficientnet.EfficientNetB0(\n","    input_shape=(224, 224, 3),\n","    include_top=False,\n","    weights='imagenet',\n","    pooling='max'\n",")\n","\n","pretrained_model.trainable = False"]},{"cell_type":"code","execution_count":null,"id":"5d3791a1","metadata":{"executionInfo":{"elapsed":73564,"status":"aborted","timestamp":1704313348503,"user":{"displayName":"Gilad Danin","userId":"00864302539574052919"},"user_tz":-120},"id":"5d3791a1"},"outputs":[],"source":["# Create checkpoint callback\n","checkpoint_path = \"birds_classification_model_checkpoint\"\n","checkpoint_callback = ModelCheckpoint(checkpoint_path,\n","                                      save_weights_only=True,\n","                                      monitor=\"val_accuracy\",\n","                                      save_best_only=True)\n","\n","# Setup EarlyStopping callback to stop training if model's val_loss doesn't improve for 3 epochs\n","early_stopping = EarlyStopping(monitor = \"val_loss\", # watch the val loss metric\n","                               patience = 5,\n","                               restore_best_weights = True) # if val loss decreases for 3 epochs in a row, stop training\n","\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)"]},{"cell_type":"markdown","id":"93ded865","metadata":{"id":"93ded865"},"source":["# üöÑTraining the model"]},{"cell_type":"code","execution_count":null,"id":"vxe_mCLBRLjO","metadata":{"executionInfo":{"elapsed":73563,"status":"aborted","timestamp":1704313348503,"user":{"displayName":"Gilad Danin","userId":"00864302539574052919"},"user_tz":-120},"id":"vxe_mCLBRLjO"},"outputs":[],"source":["inputs = pretrained_model.input\n","inputs.__dict__['_type_spec']"]},{"cell_type":"code","execution_count":null,"id":"efb61451","metadata":{"executionInfo":{"elapsed":73562,"status":"aborted","timestamp":1704313348503,"user":{"displayName":"Gilad Danin","userId":"00864302539574052919"},"user_tz":-120},"id":"efb61451"},"outputs":[],"source":["inputs = pretrained_model.input\n","x = augment(inputs)\n","x = pretrained_model(x)\n","x = Dense(128, activation='relu')(x)\n","x = Dropout(0.45)(x)\n","x = Dense(256, activation='relu')(x)\n","x = Dropout(0.45)(x)\n","\n","\n","outputs = Dense(N_LABELS, activation='softmax')(x)\n","\n","model = Model(inputs=inputs, outputs=outputs)\n","\n","model.compile(\n","    optimizer=Adam(0.0001),\n","    loss='categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n","\n"]},{"cell_type":"code","execution_count":null,"id":"73db9cab","metadata":{},"outputs":[],"source":["history = model.fit(\n","    train_images,\n","    steps_per_epoch=len(train_images),\n","    validation_data=val_images,\n","    validation_steps=len(val_images),\n","    epochs=1,\n","    callbacks=[\n","        early_stopping,\n","        # birds.create_tensorboard_callback(\"training_logs\",\n","        #                             \"bird_classification\"),\n","        checkpoint_callback,\n","        reduce_lr\n","    ]\n",")"]},{"cell_type":"markdown","id":"6d3466da","metadata":{"id":"6d3466da"},"source":["# ‚úîÔ∏èModel Evaluation\n","<div style=\"background-color:#f2f2f2; padding: 20px;\">\n","<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em\">The test dataset will be used to evaluate the performance of the model.One of the metrics that will be tested would be accuracy which measures the fraction of predictions the model got right. Other metrics are as follows:   </p>\n","\n","<h3>Precision(P):</h3>\n","<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em\">The fraction of true positives (TP, correct predictions) from the total amount of relevant results, i.e., the sum of TP and false positives (FP). For multi-class classification problems, P is averaged among the classes. The following is the formula for precision.</p>\n","\n","<h4>\n","  <center>\n","    <span style=\"font-size: 1.5em\">\n","      $P = \\frac{TP}{TP+FP}$\n","    </span>\n","  </center>\n","</h4>\n","\n","\n","<h3>Recall(R): </h3>\n","<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em\">The fraction of TP from the total amount of TP and false negatives (FN). For multi-class classification problems, R gets averaged among all the classes. The following is the formula for recall.</p>\n","\n","<h4>\n","  <center>\n","    <span style=\"font-size: 1.5em\">\n","      $R = \\frac{TP}{TP+FN}$\n","    </span>\n","  </center>\n","</h4>\n","\n","\n","<h3>F1 score(F1): </h3>\n","\n","<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em\">The harmonic mean of precision and recall. For multi-class classification problems, F1 gets averaged among all the classes. The following is the formula for F1 score.</p>\n","\n","<h4>\n","  <center>\n","    <span style=\"font-size: 1.5em\">\n","      $F1 = 2 \\times \\frac{TP \\times FP}{TP + FP}$\n","    </span>\n","  </center>\n","</h4>\n","</div>"]},{"cell_type":"code","execution_count":null,"id":"a76a20ac","metadata":{"executionInfo":{"elapsed":2,"status":"aborted","timestamp":1704313348948,"user":{"displayName":"Gilad Danin","userId":"00864302539574052919"},"user_tz":-120},"id":"a76a20ac"},"outputs":[],"source":["results = model.evaluate(test_images, verbose=0)\n","\n","print(\"    Test Loss: {:.5f}\".format(results[0]))\n","print(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))"]},{"cell_type":"markdown","id":"78d0b48d","metadata":{"id":"78d0b48d"},"source":["# üìâVisualizing loss curves"]},{"cell_type":"code","execution_count":null,"id":"710732bc","metadata":{"executionInfo":{"elapsed":2,"status":"aborted","timestamp":1704313348948,"user":{"displayName":"Gilad Danin","userId":"00864302539574052919"},"user_tz":-120},"id":"710732bc"},"outputs":[],"source":["accuracy = history.history['accuracy']\n","val_accuracy = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(len(accuracy))\n","plt.plot(epochs, accuracy, 'b', label='Training accuracy')\n","plt.plot(epochs, val_accuracy, 'r', label='Validation accuracy')\n","\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","plt.figure()\n","plt.plot(epochs, loss, 'b', label='Training loss')\n","plt.plot(epochs, val_loss, 'r', label='Validation loss')\n","\n","plt.title('Training and validation loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","id":"48f3cfad","metadata":{"id":"48f3cfad"},"source":["# üîÆMaking predictions on the Test Data"]},{"cell_type":"code","execution_count":null,"id":"1468d500","metadata":{"executionInfo":{"elapsed":2,"status":"aborted","timestamp":1704313348948,"user":{"displayName":"Gilad Danin","userId":"00864302539574052919"},"user_tz":-120},"id":"1468d500"},"outputs":[],"source":["# Predict the label of the test_images\n","pred = model.predict(test_images)\n","pred = np.argmax(pred,axis=1)\n","\n","# Map the label\n","labels = (train_images.class_indices)\n","labels = dict((v,k) for k,v in labels.items())\n","pred = [labels[k] for k in pred]\n","\n","# Display the result\n","print(f'The first 5 predictions: {pred[:5]}')"]},{"cell_type":"code","execution_count":null,"id":"a04e9b8c","metadata":{"executionInfo":{"elapsed":2,"status":"aborted","timestamp":1704313348948,"user":{"displayName":"Gilad Danin","userId":"00864302539574052919"},"user_tz":-120},"id":"a04e9b8c"},"outputs":[],"source":["  # Display 25 random pictures from the dataset with their labels\n","random_index = np.random.randint(0, len(test_df) - 1, 15)\n","fig, axes = plt.subplots(nrows=3, ncols=5, figsize=(25, 15),\n","                        subplot_kw={'xticks': [], 'yticks': []})\n","\n","for i, ax in enumerate(axes.flat):\n","    ax.imshow(plt.imread(test_df.Filepath.iloc[random_index[i]]))\n","    if test_df.Label.iloc[random_index[i]] == pred[random_index[i]]:\n","        color = \"green\"\n","    else:\n","        color = \"red\"\n","    ax.set_title(f\"True: {test_df.Label.iloc[random_index[i]]}\\nPredicted: {pred[random_index[i]]}\", color=color)\n","plt.show()\n","plt.tight_layout()"]},{"cell_type":"markdown","id":"f7f0410a","metadata":{"id":"f7f0410a"},"source":["# üìäPlotting the Classification Reports and Confusion Matrix\n","\n","<div style=\"background-color:#fff1cc; padding: 20px;\">\n","<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em\"><b>Confusion matrix</b> and <b>classification report</b> are two important tools used for evaluating the performance of an image classification model.</p>\n","\n","<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em\">A <b>confusion matrix</b> is a table that summarizes the number of correct and incorrect predictions made by a classification model on a set of test data. It is usually represented as a square matrix with rows and columns representing the predicted and true class labels, respectively. The entries of the matrix indicate the number of test samples that belong to a certain class, and how many of those were classified correctly or incorrectly by the model. A confusion matrix can provide a detailed breakdown of the performance of the model, including measures such as accuracy, precision, recall, and F1-score for each class. It can be used to identify specific areas where the model is making errors, and to diagnose problems with the model's predictions.</p>\n","\n","<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em\">A <b>classification report</b> is a summary of the key performance metrics for a classification model, including precision, recall, and F1-score, as well as the overall accuracy of the model. It provides a concise overview of the model's performance, typically broken down by class, and can be used to quickly assess the strengths and weaknesses of the model. The report is often presented as a table, with each row representing a class and columns showing various performance metrics. The report may also include other metrics such as support (the number of test samples belonging to a particular class), and the macro- and micro-averages of the performance metrics across all classes.</p>\n","\n","<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em\">In image classification, both confusion matrix and classification report are important tools for evaluating the performance of the model, identifying areas for improvement, and making decisions about how to adjust the model's architecture or training parameters.</p>\n","</div>"]},{"cell_type":"code","execution_count":null,"id":"fea0d3c7","metadata":{"executionInfo":{"elapsed":2,"status":"aborted","timestamp":1704313348948,"user":{"displayName":"Gilad Danin","userId":"00864302539574052919"},"user_tz":-120},"id":"fea0d3c7"},"outputs":[],"source":["y_test = list(test_df.Label)\n","print(classification_report(y_test, pred))"]},{"cell_type":"code","execution_count":null,"id":"b0aabc83","metadata":{"executionInfo":{"elapsed":2,"status":"aborted","timestamp":1704313348948,"user":{"displayName":"Gilad Danin","userId":"00864302539574052919"},"user_tz":-120},"id":"b0aabc83"},"outputs":[],"source":["report = classification_report(y_test, pred, output_dict=True)\n","df = pd.DataFrame(report).transpose()\n","df"]},{"cell_type":"markdown","id":"08625c9d","metadata":{"id":"08625c9d"},"source":["# ‚òÄÔ∏èGrad-Cam Visualization\n","\n","<div style=\"background-color:#fff1cc; padding: 20px;\">\n","<p style=\"font-size:20px; font-family:verdana; line-height: 1.7em\"><b>Grad-CAM (Gradient-weighted Class Activation Mapping)</b> is a technique used to visualize the regions of an input image that were most relevant for a neural network's prediction. It allows you to see which regions of the image the model focused on while making its prediction. Grad-CAM is a modification of the CAM technique that extends the latter to any model that uses a convolutional neural network (CNN) as its underlying architecture.</p>\n","</div>"]},{"cell_type":"code","execution_count":null,"id":"05f6d514","metadata":{"executionInfo":{"elapsed":2,"status":"aborted","timestamp":1704313348948,"user":{"displayName":"Gilad Danin","userId":"00864302539574052919"},"user_tz":-120},"id":"05f6d514"},"outputs":[],"source":["def get_img_array(img_path, size):\n","    img = tf.keras.preprocessing.image.load_img(img_path, target_size=size)\n","    array = tf.keras.preprocessing.image.img_to_array(img)\n","    # We add a dimension to transform our array into a \"batch\"\n","    # of size \"size\"\n","    array = np.expand_dims(array, axis=0)\n","    return array\n","\n","def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n","    # First, we create a model that maps the input image to the activations\n","    # of the last conv layer as well as the output predictions\n","    grad_model = tf.keras.models.Model(\n","        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n","    )\n","\n","    # Then, we compute the gradient of the top predicted class for our input image\n","    # with respect to the activations of the last conv layer\n","    with tf.GradientTape() as tape:\n","        last_conv_layer_output, preds = grad_model(img_array)\n","        if pred_index is None:\n","            pred_index = tf.argmax(preds[0])\n","        class_channel = preds[:, pred_index]\n","    # This is the gradient of the output neuron (top predicted or chosen)\n","    # with regard to the output feature map of the last conv layer\n","    grads = tape.gradient(class_channel, last_conv_layer_output)\n","\n","    # This is a vector where each entry is the mean intensity of the gradient\n","    # over a specific feature map channel\n","    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n","\n","    # We multiply each channel in the feature map array\n","    # by \"how important this channel is\" with regard to the top predicted class\n","    # then sum all the channels to obtain the heatmap class activation\n","    last_conv_layer_output = last_conv_layer_output[0]\n","    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n","    heatmap = tf.squeeze(heatmap)\n","\n","    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n","    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n","    return heatmap.numpy()\n","def save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n","    # Load the original image\n","    img = tf.keras.preprocessing.image.load_img(img_path)\n","    img = tf.keras.preprocessing.image.img_to_array(img)\n","\n","    # Rescale heatmap to a range 0-255\n","    heatmap = np.uint8(255 * heatmap)\n","\n","    # Use jet colormap to colorize heatmap\n","    jet = cm.get_cmap(\"jet\")\n","\n","    # Use RGB values of the colormap\n","    jet_colors = jet(np.arange(256))[:, :3]\n","    jet_heatmap = jet_colors[heatmap]\n","\n","    # Create an image with RGB colorized heatmap\n","    jet_heatmap = tf.keras.preprocessing.image.array_to_img(jet_heatmap)\n","    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n","    jet_heatmap = tf.keras.preprocessing.image.img_to_array(jet_heatmap)\n","\n","    # Superimpose the heatmap on original image\n","    superimposed_img = jet_heatmap * alpha + img\n","    superimposed_img = tf.keras.preprocessing.image.array_to_img(superimposed_img)\n","    # Save the superimposed image\n","    superimposed_img.save(cam_path)\n","\n","    # Display Grad CAM\n","#     display(Image(cam_path))\n","\n","    return cam_path\n","\n","\n","preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n","decode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions\n","\n","last_conv_layer_name = \"top_conv\"\n","img_size = (224,224, 3)\n","\n","# Remove last layer's softmax\n","model.layers[-1].activation = None"]},{"cell_type":"code","execution_count":null,"id":"a447eb1b","metadata":{"executionInfo":{"elapsed":2,"status":"aborted","timestamp":1704313348948,"user":{"displayName":"Gilad Danin","userId":"00864302539574052919"},"user_tz":-120},"id":"a447eb1b"},"outputs":[],"source":["# Display the part of the pictures used by the neural network to classify the pictures\n","fig, axes = plt.subplots(nrows=3, ncols=5, figsize=(15, 10),\n","                        subplot_kw={'xticks': [], 'yticks': []})\n","\n","for i, ax in enumerate(axes.flat):\n","    img_path = test_df.Filepath.iloc[random_index[i]]\n","    img_array = preprocess_input(get_img_array(img_path, size=img_size))\n","    heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n","    cam_path = save_and_display_gradcam(img_path, heatmap)\n","    ax.imshow(plt.imread(cam_path))\n","    ax.set_title(f\"True: {test_df.Label.iloc[random_index[i]]}\\nPredicted: {pred[random_index[i]]}\")\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","id":"482400a8","metadata":{"id":"482400a8"},"source":["***\n","\n","<div style=\"color:white;\n","           display:fill;\n","           border-radius:5px;\n","           background-color:#5642C5;\n","           font-size:110%;\n","           font-family:Verdana;\n","           letter-spacing:0.5px\">\n","        <p style=\"padding: 10px;\n","              color:white;\">\n","            Thanks for viewing my work. If you like it, consider sharing it to others or give feedback to improve the notebook. Have a beautiful day my friend.\n","        </p>\n","    </div>\n","\n","<center><img src='https://media4.giphy.com/media/M9gbBd9nbDrOTu1Mqx/giphy.gif?cid=790b7611704aa2ca4e403287801480a6c753abf45f3e6242&rid=giphy.gif&ct=s'\n","     height=30px width=160px /></center>"]}],"metadata":{"colab":{"collapsed_sections":["eaf7aa03"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"papermill":{"default_parameters":{},"duration":17690.016081,"end_time":"2023-04-26T10:36:46.672724","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-04-26T05:41:56.656643","version":"2.3.4"}},"nbformat":4,"nbformat_minor":5}
